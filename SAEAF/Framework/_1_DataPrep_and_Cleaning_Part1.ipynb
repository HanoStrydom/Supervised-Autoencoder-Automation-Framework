{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0cb8b421",
   "metadata": {},
   "source": [
    "### Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7109753",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" All modules for this steps of the pipeline are defined here. \"\"\"\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786393d9",
   "metadata": {},
   "source": [
    "### Import Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e70273c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Loading environment variables from .env file \"\"\"\n",
    "load_dotenv()\n",
    "\n",
    "# Load the dataset\n",
    "original_dataset = os.getenv('ORIGINAL_DATASET')\n",
    "\n",
    "drop_columns = os.getenv(\"drop_columns\", '')\n",
    "print(f\"drop_columns: {drop_columns}\")\n",
    "userOption = os.getenv('missing_choice', \"1\")\n",
    "default_value = os.getenv('DefaultMissingValue', 'Unknown')\n",
    "convert_categorical = os.getenv('convert_categorical', '1')\n",
    "\n",
    "mean_cols = os.getenv('statistical_measure_mean', '')\n",
    "mode_cols = os.getenv('statistical_measure_mode', '')\n",
    "\n",
    "mean_cols = [col.strip() for col in mean_cols.split(',') if col.strip()]\n",
    "mode_cols = [col.strip() for col in mode_cols.split(',') if col.strip()]\n",
    "\n",
    "ignore_columns = os.getenv(\"ignore_columns\", \"\").split(\",\")\n",
    "ignore_columns = [col.strip() for col in ignore_columns]\n",
    "\n",
    "Scaling = os.getenv(\"Scaling\", \"0\")\n",
    "NormalScalingColumns = os.getenv(\"NormalScalingColumns\", \"\")\n",
    "StandardScalingColumns = os.getenv(\"StandardScalingColumns\", \"\")\n",
    "\n",
    "normal_cols = [col.strip() for col in NormalScalingColumns.split(\",\") if col.strip()]\n",
    "standard_cols = [col.strip() for col in StandardScalingColumns.split(\",\") if col.strip()]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b6a317",
   "metadata": {},
   "source": [
    "### Data Preparation and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd4f649",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Function to drop specified columns from the dataframe.\"\"\"\n",
    "\n",
    "def drop_columns_from_df(filePath):\n",
    "    # Load the file based on extension\n",
    "    if filePath.endswith('.xlsx'):\n",
    "        df_dropped = pd.read_excel(filePath)\n",
    "    elif filePath.endswith('.csv'):\n",
    "        df_dropped = pd.read_csv(filePath)\n",
    "    else:\n",
    "        print(\"Unsupported file format. Please use a .xlsx or .csv file.\")\n",
    "        return None\n",
    "\n",
    "    print(\"Shape after loading:\", df_dropped.shape)\n",
    "\n",
    "    # Process drop_columns from global .env-style string\n",
    "    if drop_columns:\n",
    "        try:\n",
    "            columns = [col.strip() for col in drop_columns.split(\",\")]\n",
    "            columns_to_drop = [col for col in columns if col in df_dropped.columns]\n",
    "\n",
    "            if columns_to_drop:\n",
    "                df_dropped = df_dropped.drop(columns=columns_to_drop)\n",
    "                print(f\"Columns dropped: {columns_to_drop}\")\n",
    "            else:\n",
    "                print(\"No matching columns found to drop.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error while dropping columns: {e}\")\n",
    "            return None\n",
    "    else:\n",
    "        print(\"No columns specified for dropping.\")\n",
    "\n",
    "    # Reset index to avoid any alignment issues downstream\n",
    "    df_dropped = df_dropped.reset_index(drop=True)\n",
    "    print(\"Shape after dropping columns and resetting index:\", df_dropped.shape)\n",
    "\n",
    "    return df_dropped\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7ea708",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Function to handle missing data in the dataframe using various techniques.\"\"\"\n",
    "\n",
    "def handle_missing_data(df):\n",
    "    print(f\"User Option: {userOption}\")\n",
    "    \n",
    "    # Replace empty strings or whitespaces with NaN first\n",
    "    df = df.replace(r'^\\s*$', np.nan, regex=True)\n",
    "\n",
    "    conversion_feedback = []\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == 'object':\n",
    "            converted = pd.to_numeric(df[col], errors='coerce')\n",
    "            if not converted.isna().all():\n",
    "                df[col] = converted\n",
    "                conversion_feedback.append(f\"Converted column '{col}' to numeric (float).\")\n",
    "            else:\n",
    "                conversion_feedback.append(f\"Column '{col}' remains as object (non-numeric values detected).\")\n",
    "\n",
    "    if userOption == \"1\":\n",
    "        # Drop rows with any missing values\n",
    "        print(\"Dropping rows with empty or missing values\")\n",
    "        df_cleaned = df.dropna()\n",
    "\n",
    "    elif userOption == \"2\":\n",
    "        # Use default value from .env\n",
    "        print(f\"Using default value selected by user: {default_value}\")\n",
    "        df_cleaned = df.fillna(default_value)\n",
    "\n",
    "    elif userOption == \"3\":\n",
    "        print(\"Using statistical measures from .env\")\n",
    "        df_cleaned = df.copy()\n",
    "\n",
    "        # Impute missing values column by column\n",
    "        for col in df.columns:\n",
    "            if not df[col].isnull().any():\n",
    "                continue  # Skip if no missing values\n",
    "\n",
    "            # Use mean if specified\n",
    "            if col in mean_cols:\n",
    "                fill_val = df[col].astype(float).mean()\n",
    "                method = 'mean'\n",
    "\n",
    "            # Use mode if specified\n",
    "            elif col in mode_cols:\n",
    "                fill_val = df[col].mode()[0]\n",
    "                method = 'mode'\n",
    "\n",
    "            # Fallback: object → mode, numeric → mean\n",
    "            else:\n",
    "                if df[col].dtype == 'object':\n",
    "                    fill_val = df[col].mode()[0]\n",
    "                    method = 'mode (fallback)'\n",
    "                else:\n",
    "                    fill_val = df[col].astype(float).mean()\n",
    "                    method = 'mean (fallback)'\n",
    "\n",
    "            df_cleaned[col] = df[col].fillna(fill_val)\n",
    "            print(f\"Filling missing values in column '{col}' with {method}: {fill_val}\")\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Option must be 1 (drop), 2 (default), or 3 (statistical)\")\n",
    "\n",
    "    # Reset index to avoid downstream misalignment\n",
    "    df_cleaned = df_cleaned.reset_index(drop=True)\n",
    "\n",
    "    return df_cleaned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301a3b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Function to check missing values \"\"\"\n",
    "\n",
    "def FindMissingValues(df):\n",
    "\n",
    "    # Get missing values location\n",
    "    missingValues = df.isnull()\n",
    "    missingLocations = []\n",
    "\n",
    "    for row in range(len(missingValues)):\n",
    "        for col in range(len(missingValues.columns)):\n",
    "            if missingValues.iat[row, col]:\n",
    "                cellName = f\"{missingValues.columns[col]}{row + 2}\"\n",
    "                missingLocations.append((row + 2, col + 1, cellName))\n",
    "\n",
    "    # Display Result\n",
    "    if missingLocations:\n",
    "        print(\"Missing values found at the following locations: \")\n",
    "        for location in missingLocations:\n",
    "            print(f\"Row: {location[0]}, Column: {location[1]}, Cell: {location[2]}\")\n",
    "        \n",
    "        df_cleaned = handle_missing_data(df)\n",
    "        return df_cleaned\n",
    "    else:\n",
    "        print(\"No missing value found in the file\")\n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3654f4",
   "metadata": {},
   "source": [
    "### Feature Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee60109",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Function to convert categorical variables to numerical \"\"\"\n",
    "\n",
    "def convertCategoricalToNumerical(df):\n",
    "    df_converted = pd.DataFrame()\n",
    "    label_encoder = LabelEncoder()\n",
    "\n",
    "    for column in df.columns:\n",
    "        if column in ignore_columns:\n",
    "            # Keep column as-is\n",
    "            df_converted[column] = df[column]\n",
    "\n",
    "        elif df[column].dtype in ['object', 'category']:\n",
    "            unique_values = df[column].nunique()\n",
    "\n",
    "            if unique_values == 2:\n",
    "                # Label encode binary columns\n",
    "                df_converted[column] = label_encoder.fit_transform(df[column])\n",
    "            else:\n",
    "                # One-hot encode and add all dummy columns at once\n",
    "                dummies = pd.get_dummies(df[column], prefix=column)\n",
    "                df_converted = pd.concat([df_converted, dummies], axis=1)\n",
    "        else:\n",
    "            # Keep numeric columns as-is\n",
    "            df_converted[column] = df[column]\n",
    "\n",
    "    # Optional: defragment memory\n",
    "    df_converted = df_converted.copy()\n",
    "\n",
    "    return df_converted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197b7fe5",
   "metadata": {},
   "source": [
    "### Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34d3579",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Function to scale data \"\"\"\n",
    "\n",
    "def ScaleData(df):\n",
    "\n",
    "    df_scaled = df.copy()\n",
    "\n",
    "    if Scaling == \"0\":\n",
    "        print(\"No scaling applied.\")\n",
    "        return df_scaled\n",
    "\n",
    "    # Apply Min-Max Scaling\n",
    "    if Scaling in [\"1\", \"3\"] and normal_cols:\n",
    "        minmax_scaler = MinMaxScaler()\n",
    "        cols_to_scale = [col for col in normal_cols if col in df_scaled.columns]\n",
    "        if cols_to_scale:\n",
    "            df_scaled[cols_to_scale] = minmax_scaler.fit_transform(df_scaled[cols_to_scale])\n",
    "            print(f\"Min-Max scaling applied to columns: {cols_to_scale}\")\n",
    "        else:\n",
    "            print(\"No matching columns found for Min-Max scaling.\")\n",
    "\n",
    "    # Apply Z-score Standardisation\n",
    "    if Scaling in [\"2\", \"3\"] and standard_cols:\n",
    "        zscore_scaler = StandardScaler()\n",
    "        cols_to_scale = [col for col in standard_cols if col in df_scaled.columns]\n",
    "        if cols_to_scale:\n",
    "            df_scaled[cols_to_scale] = zscore_scaler.fit_transform(df_scaled[cols_to_scale])\n",
    "            print(f\"Z-score scaling applied to columns: {cols_to_scale}\")\n",
    "        else:\n",
    "            print(\"No matching columns found for Z-score scaling.\")\n",
    "\n",
    "    return df_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c28ed97",
   "metadata": {},
   "source": [
    "### Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0097ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Function to read data from the original dataset path \"\"\"\n",
    "\n",
    "def readData():\n",
    "    filePath = f\"{original_dataset}\"\n",
    "    print(filePath)\n",
    "\n",
    "    return filePath"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba8ea1c",
   "metadata": {},
   "source": [
    "### Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3f14b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Main function to run data preparation steps \"\"\"\n",
    "\n",
    "def run_data_prep():\n",
    "    filePath = readData()\n",
    "\n",
    "    dataFrame = drop_columns_from_df(filePath)\n",
    "\n",
    "    # Replaces / Removes missing values\n",
    "    dataFrame = FindMissingValues(dataFrame)\n",
    "    \n",
    "    # Convert categorical variables to numerical\n",
    "    if convert_categorical == \"1\":\n",
    "        \n",
    "        dataFrame = convertCategoricalToNumerical(dataFrame)\n",
    "        print(\"Categorical variables converted to numerical.\")\n",
    "\n",
    "    # Scale data if specified\n",
    "    dataFrame = ScaleData(dataFrame)\n",
    "\n",
    "    print(\"Data preparation completed.\")\n",
    "    return dataFrame \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a69ce73",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    run_data_prep()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Masters",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
