{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b9419d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" All modules for this steps of the pipeline are defined here. \"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay, roc_auc_score, average_precision_score, RocCurveDisplay, PrecisionRecallDisplay\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from _4_ModelBuildingTrainingAndHyperparameterOptimisation import ModelBuildAndTrain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c64629",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seeds\n",
    "\n",
    "\"\"\" Loading environment variables from .env file\"\"\"\n",
    "load_dotenv()\n",
    "\n",
    "base_path: str = os.getenv('base_path')\n",
    "trials: str = os.getenv('trials')\n",
    "trial_models: str = os.getenv('trial_models')\n",
    "results: str = os.getenv('results')\n",
    "model: str = os.getenv('model')\n",
    "results: str = os.getenv('results')\n",
    "amount_of_trials: int = int(os.getenv('amount_of_trials'))\n",
    "split = os.getenv('split')\n",
    "\n",
    "conf_title = os.getenv('conf_title')\n",
    "conf_xlabel = os.getenv('conf_xlabel')\n",
    "conf_ylabel = os.getenv('conf_ylabel')\n",
    "\n",
    "# Print to check if paths are loaded correctly\n",
    "print(f\"Base Path: {base_path}\")\n",
    "print(f\"Trials Path: {trials}\")\n",
    "print(f\"Trials Models Path: {trial_models}\")\n",
    "print(f\"Results Path: {results}\")\n",
    "print(f\"Model Path: {model}\")\n",
    "print(f\"Results Path: {results}\")\n",
    "print(f\"Amount of Trials: {amount_of_trials}\")\n",
    "print(f\"Split: {split}\")\n",
    "\n",
    "if(conf_title == \"\"):\n",
    "    conf_title = \"Confusion Matrix\"\n",
    "if(conf_xlabel == \"\"):\n",
    "    conf_xlabel = \"Predicted Label\"\n",
    "if(conf_ylabel == \"\"):\n",
    "    conf_ylabel = \"True Label\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad5ab7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Function to test the trained model and save performance metrics and plots. \"\"\"\n",
    "\n",
    "def Test_Model(x_test_path, y_test_path, model_path):\n",
    "    # Load the testing data\n",
    "    X_test = pd.read_csv(x_test_path)\n",
    "    y_test = pd.read_csv(y_test_path)\n",
    "\n",
    "    model_loaded = tf.keras.models.load_model(model_path)\n",
    "\n",
    "    # Convert y_test to NumPy array\n",
    "    y_test_array = y_test.values\n",
    "    y_test_categorical = y_test_array\n",
    "\n",
    "    print(\"Testing the model...\")\n",
    "\n",
    "    # Evaluate the model\n",
    "    losses = model_loaded.evaluate(X_test, [X_test, y_test_categorical])\n",
    "    reconstruction_loss = losses[0]\n",
    "    classification_loss = losses[1]\n",
    "\n",
    "    # Predict probabilities for the test set\n",
    "    y_pred_prob = model_loaded.predict(X_test)[1]\n",
    "\n",
    "    # Apply threshold to convert probabilities to binary labels\n",
    "    threshold = 0.5\n",
    "    y_pred_labels = (y_pred_prob > threshold).astype(int)\n",
    "\n",
    "    classification_accuracy = accuracy_score(y_test_categorical, y_pred_labels)\n",
    "\n",
    "    print(\"Reconstruction Loss:\", reconstruction_loss)\n",
    "    print(\"Classification Loss:\", classification_loss)\n",
    "    print(\"Classification Accuracy:\", classification_accuracy)\n",
    "\n",
    "    # Generate confusion matrix and classification report\n",
    "    conf_matrix = confusion_matrix(y_test_categorical, y_pred_labels)\n",
    "    class_report = classification_report(y_test_categorical, y_pred_labels)\n",
    "    roc_auc = roc_auc_score(y_test_categorical, y_pred_prob)\n",
    "    pr_auc = average_precision_score(y_test_categorical, y_pred_prob)\n",
    "\n",
    "    # Ensure model folder exists\n",
    "    os.makedirs(model, exist_ok=True)\n",
    "\n",
    "    # Save performance metrics to performance_metrics.txt\n",
    "    performance_metrics_path = os.path.join(model, \"performance_metrics.txt\")\n",
    "    with open(performance_metrics_path, \"w\") as f:\n",
    "        f.write(f\"Reconstruction Loss: {reconstruction_loss}\\n\")\n",
    "        f.write(f\"Classification Loss: {classification_loss}\\n\")\n",
    "        f.write(f\"Classification Accuracy: {classification_accuracy}\\n\")\n",
    "        f.write(f\"Confusion Matrix:\\n{conf_matrix}\\n\\n\")\n",
    "        f.write(f\"Classification Report:\\n{class_report}\\n\")\n",
    "        f.write(f\"ROC AUC: {roc_auc}\\n\")\n",
    "        f.write(f\"PR AUC: {pr_auc}\\n\")\n",
    "\n",
    "    # Save predictions comparison to predictions_comparison.txt\n",
    "    predictions_path = os.path.join(model, \"predictions_comparison.txt\")\n",
    "    with open(predictions_path, \"w\") as f:\n",
    "        for actual, predicted in zip(y_test_categorical, y_pred_prob):\n",
    "            f.write(f\"[{int(actual[0])}]  {predicted[0]:.2f}\\n\")\n",
    "\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=[0, 1])\n",
    "    disp.plot(cmap=plt.cm.Blues)\n",
    "    plt.title(conf_title)\n",
    "    plt.xlabel(conf_xlabel)\n",
    "    plt.ylabel(conf_ylabel)\n",
    "\n",
    "    # Save the figure to the model directory\n",
    "    confusion_matrix_fig_path = os.path.join(model, \"confusion_matrix.png\")\n",
    "    plt.savefig(confusion_matrix_fig_path)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    # ROC Curve Plot\n",
    "    roc_display = RocCurveDisplay.from_predictions(y_test_categorical, y_pred_prob)\n",
    "    plt.title(\"ROC Curve\")\n",
    "    roc_fig_path = os.path.join(model, \"roc_curve.png\")\n",
    "    plt.savefig(roc_fig_path)\n",
    "    plt.show()\n",
    "\n",
    "    # Precision-Recall Curve Plot\n",
    "    pr_display = PrecisionRecallDisplay.from_predictions(y_test_categorical, y_pred_prob)\n",
    "    plt.title(\"Precision-Recall Curve\")\n",
    "    pr_fig_path = os.path.join(model, \"precision_recall_curve.png\")\n",
    "    plt.savefig(pr_fig_path)\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Performance metrics saved to: {performance_metrics_path}\")\n",
    "    print(f\"Predictions comparison saved to: {predictions_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b618191",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Main function to build and train the model with hyperparameter optimization. \"\"\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "   \n",
    "   best_model_path = ModelBuildAndTrain()\n",
    "   \n",
    "   # Example usage:\n",
    "   x_test_path = os.path.join(split, 'X_Test.csv')\n",
    "   y_test_path = os.path.join(split, 'Y_Test.csv')\n",
    "\n",
    "   model_path = best_model_path\n",
    "   \n",
    "   Test_Model(x_test_path, y_test_path, model_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Masters",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
