{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "974962ee",
   "metadata": {},
   "source": [
    "### Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38221a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" All modules for this steps of the pipeline are defined here. \"\"\"\n",
    "\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical, plot_model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from dotenv import load_dotenv\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from PIL import Image\n",
    "import optuna\n",
    "import random\n",
    "from tensorflow.keras.metrics import AUC\n",
    "\n",
    "from _3_DataPrep_and_Cleaning_Part2 import run_data_prep_part2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb29a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "global best_model_path\n",
    "best_model_path = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44755ff",
   "metadata": {},
   "source": [
    "### Check GPU Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b046a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for GPU availability\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    print(\"GPUs available:\", gpus)\n",
    "else:\n",
    "    print(\"No GPUs found. Training will use CPU.\")\n",
    "\n",
    "# Set memory growth for GPU\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a2247f",
   "metadata": {},
   "source": [
    "### Import Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8540051",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seeds\n",
    "\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "\"\"\" Loading environment variables from .env file\"\"\"\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "base_path: str = os.getenv('base_path')\n",
    "trials: str = os.getenv('trials')\n",
    "trial_models: str = os.getenv('trial_models')\n",
    "results: str = os.getenv('results')\n",
    "model: str = os.getenv('model')\n",
    "results: str = os.getenv('results')\n",
    "amount_of_trials: int = int(os.getenv('amount_of_trials'))\n",
    "autoencoder_loss = os.getenv('autoencoder_Loss')\n",
    "autoencoder_metric = os.getenv('autoencoder_Metric')\n",
    "mlp_loss = os.getenv('mlp_Loss')\n",
    "mlp_metric = os.getenv('mlp_Metric')\n",
    "\n",
    "# Print to check if paths are loaded correctly\n",
    "print(f\"Base Path: {base_path}\")\n",
    "print(f\"Trials Path: {trials}\")\n",
    "print(f\"Trials Models Path: {trial_models}\")\n",
    "print(f\"Results Path: {results}\")\n",
    "print(f\"Model Path: {model}\")\n",
    "\n",
    "if amount_of_trials == 0:\n",
    "    amount_of_trials = 500\n",
    "\n",
    "# Fallback to default if they are empty or None\n",
    "\n",
    "autoencoder_loss = autoencoder_loss.strip() if autoencoder_loss else 'mse'\n",
    "autoencoder_metric = autoencoder_metric.strip() if autoencoder_metric else 'mse'\n",
    "mlp_loss = mlp_loss.strip() if mlp_loss else 'binary_crossentropy'\n",
    "mlp_metric = mlp_metric.strip() if mlp_metric else 'accuracy'\n",
    "\n",
    "# Convert comma-separated strings to lists\n",
    "autoencoder_metric = [m.strip() for m in autoencoder_metric.split(',')] if ',' in autoencoder_metric else [autoencoder_metric]\n",
    "mlp_metric = [m.strip() for m in mlp_metric.split(',')] if ',' in mlp_metric else [mlp_metric]\n",
    "\n",
    "# Optionally handle 'auc' keyword for MLP metrics\n",
    "\n",
    "mlp_metric = [AUC(name='auc') if m.lower() == 'auc' else m for m in mlp_metric]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f68c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Function to load training, validation, and testing data from CSV files.\"\"\"\n",
    "\n",
    "def data():\n",
    "\n",
    "    # Load features using the base path\n",
    "    X_train = pd.read_csv(f'{base_path}x_train.csv')\n",
    "    X_val = pd.read_csv(f'{base_path}x_val.csv')\n",
    "    X_test = pd.read_csv(f'{base_path}x_test.csv')\n",
    "\n",
    "    # Load targets using the base path\n",
    "    y_train = pd.read_csv(f'{base_path}y_train.csv')\n",
    "    y_val = pd.read_csv(f'{base_path}y_val.csv')\n",
    "    y_test = pd.read_csv(f'{base_path}y_test.csv')\n",
    "\n",
    "    # Debug statements to check the shapes of the loaded data\n",
    "    print('X_train shape:', X_train.shape)\n",
    "    print('X_val shape:', X_val.shape)\n",
    "    print('X_test shape:', X_test.shape)\n",
    "\n",
    "    print('y_train shape:', y_train.shape)\n",
    "    print('y_val shape:', y_val.shape)\n",
    "    print('y_test shape:', y_test.shape)\n",
    "\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5136ff24",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Function to plot and save model architecture and training results. \"\"\"\n",
    "\n",
    "def plot_results(history, supervised_autoencoder, location, trial_number):\n",
    "    # Plot the model\n",
    "    folder_path = location        \n",
    "\n",
    "    # Create a new folder for the trial if it doesn't exist\n",
    "    folder_name = \"Trial_\" + str(trial_number)\n",
    "    new_folder_path = os.path.join(folder_path, folder_name)\n",
    "    if not os.path.exists(new_folder_path):\n",
    "        os.makedirs(new_folder_path)\n",
    "        \n",
    "    plot_model(supervised_autoencoder, to_file=os.path.join(new_folder_path, 'Model.png'), show_shapes=True, show_layer_names=True, expand_nested=True)\n",
    "    print(\"Model saved to: \", new_folder_path)\n",
    "    # Model Training and Validation loss\n",
    "    plt.figure()\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.title('Model loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend('Train', loc='upper left')\n",
    "    plt.savefig(os.path.join(new_folder_path, 'Loss.png'))\n",
    "\n",
    "    # Autoencoder Training and Validation accuracy\n",
    "    plt.figure()\n",
    "    plt.plot(history.history['MLP_Output_accuracy'])\n",
    "    plt.plot(history.history['val_MLP_Output_accuracy'])\n",
    "    plt.title('Model accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "    plt.savefig(os.path.join(new_folder_path, 'MLP_Acc.png'))\n",
    "\n",
    "    # Autoencoder MSE\n",
    "    plt.figure()\n",
    "    plt.plot(history.history['Autoencoder_Output_mse'])\n",
    "    plt.plot(history.history['val_Autoencoder_Output_mse'])\n",
    "    plt.title('Autoencoder MSE')\n",
    "    plt.ylabel('MSE')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "    plt.savefig(os.path.join(new_folder_path, 'MSE.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bda2105",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Function to save hyperparameters of the model trial. \"\"\"\n",
    "\n",
    "def save_hyperparameters(trial_number, learning_rate, num_epochs, num_layers_auto, num_layers_mlp, \n",
    "                         nodes_per_layer_autoencoder, nodes_per_layer_mlp, use_dropout, dropout_rate, codeNodes):\n",
    "    # Create a folder for the model\n",
    "    model_folder = f'{trial_models}/Model_{trial_number}/'\n",
    "    os.makedirs(model_folder, exist_ok=True)\n",
    "    \n",
    "    # Save hyperparameters to a text file\n",
    "    hyperparams_file = os.path.join(model_folder, 'hyperparameters.txt')\n",
    "    with open(hyperparams_file, 'w') as f:\n",
    "        f.write(f\"Trial Number: {trial_number}\\n\")\n",
    "        f.write(f\"Learning Rate: {learning_rate}\\n\")\n",
    "        f.write(f\"Number of Epochs: {num_epochs}\\n\")\n",
    "        f.write(f\"Number of Layers (Autoencoder): {num_layers_auto}\\n\")\n",
    "        f.write(f\"Nodes per Layer (Autoencoder): {nodes_per_layer_autoencoder}\\n\")\n",
    "        f.write(f\"Number of Layers (MLP): {num_layers_mlp}\\n\")\n",
    "        f.write(f\"Nodes per Layer (MLP): {nodes_per_layer_mlp}\\n\")\n",
    "        f.write(f\"Use Dropout: {use_dropout}\\n\")\n",
    "        f.write(f\"Dropout Rate: {dropout_rate}\\n\")\n",
    "        f.write(f\"Code Layer Nodes: {codeNodes}\\n\")\n",
    "    print(f\"Saved hyperparameters as {hyperparams_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba70916",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Function to save the model architecture as a PNG image. \"\"\"\n",
    "\n",
    "def save_model_architecture(supervised_autoencoder, trial_number):\n",
    "    model_folder = f'{trial_models}/Model_{trial_number}/'\n",
    "    os.makedirs(model_folder, exist_ok=True)\n",
    "    \n",
    "    # Save the model architecture as a PNG image\n",
    "    architecture_file = os.path.join(model_folder, 'model_architecture.png')\n",
    "    plot_model(supervised_autoencoder, to_file=architecture_file, show_shapes=True, show_layer_names=True)\n",
    "    print(f\"Saved model architecture as {architecture_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ca315f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Function to plot and save training and validation accuracy and loss. \"\"\"\n",
    "\n",
    "def save_plots(history, trial_number, model_type):\n",
    "    model_folder = f'{trial_models}/Model_{trial_number}/'\n",
    "    os.makedirs(model_folder, exist_ok=True)\n",
    "    \n",
    "    acc = history.history['MLP_Output_accuracy']\n",
    "    val_acc = history.history['val_MLP_Output_accuracy']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    \n",
    "    # Plot accuracy\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(acc, label='Training Accuracy')\n",
    "    plt.plot(val_acc, label='Validation Accuracy')\n",
    "    plt.title(f'Training and Validation Accuracy - Model {trial_number}')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    accuracy_plot_path = f'{model_folder}{model_type}_accuracy_plot.png'\n",
    "    plt.savefig(accuracy_plot_path)\n",
    "    plt.close()\n",
    "\n",
    "    # Plot loss\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(loss, label='Training Loss')\n",
    "    plt.plot(val_loss, label='Validation Loss')\n",
    "    plt.title(f'Training and Validation Loss - Model {trial_number}')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    loss_plot_path = f'{model_folder}{model_type}_loss_plot.png'\n",
    "    plt.savefig(loss_plot_path)\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"Saved accuracy plot as {accuracy_plot_path}\")\n",
    "    print(f\"Saved loss plot as {loss_plot_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6d04c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the Supervised Autoencoder with additional model saving features\n",
    "def build_supervised_autoencoder(X_train, X_val, y_train, y_val, learning_rate, num_epochs, num_layers_auto, num_layers_mlp, \n",
    "                                 nodes_per_layer_autoencoder, nodes_per_layer_mlp, trial_number, use_dropout, dropout_rate, codeNodes):\n",
    "\n",
    "    print(\"After reshaping:\")\n",
    "    print(\"X_train shape:\", X_train.shape)\n",
    "    print(\"y_train shape:\", y_train.shape)\n",
    "    print(\"X_val shape:\", X_val.shape)\n",
    "    print(\"y_val shape:\", y_val.shape)\n",
    "    \n",
    "    input_layer = Input(shape=(X_train.shape[1],), name=\"Input\")\n",
    "\n",
    "    # Encoder part stays the same\n",
    "    encoder_layer = input_layer\n",
    "    for i in range(num_layers_auto):\n",
    "        encoder_layer = Dense(nodes_per_layer_autoencoder[i], activation=\"relu\", name=\"Encode_layer_\" + str(i))(encoder_layer)\n",
    "        if use_dropout and i % 3 == 0:\n",
    "            encoder_layer = Dropout(dropout_rate, name=\"dropout_layer_encoder_\" + str(i))(encoder_layer)\n",
    "\n",
    "    code_layer = Dense(codeNodes, activation=\"relu\", name=\"Code_Layer\")(encoder_layer)\n",
    "\n",
    "    # Decoder part (reversed structure)\n",
    "    decoder_layer = code_layer\n",
    "    for i in reversed(range(num_layers_auto)):\n",
    "        decoder_layer = Dense(nodes_per_layer_autoencoder[i], activation=\"relu\", name=\"Decode_layer_\" + str(i))(decoder_layer)\n",
    "        if use_dropout and i % 3 == 0:\n",
    "            decoder_layer = Dropout(dropout_rate, name=\"dropout_layer_decoder_\" + str(i))(decoder_layer)\n",
    "\n",
    "    # Output layer\n",
    "    output_layer = Dense(X_train.shape[1], activation=\"linear\", name=\"Autoencoder_Output\")(decoder_layer)\n",
    "\n",
    "\n",
    "    # Classifier\n",
    "    MLP_Layer = code_layer\n",
    "    for i in range(num_layers_mlp):\n",
    "        MLP_Layer = Dense(nodes_per_layer_mlp[i], activation=\"relu\", name=\"MLP_\" + str(i))(MLP_Layer)\n",
    "        \n",
    "    # Classifier Output for Binary Classification\n",
    "    output = Dense(1, activation=\"sigmoid\", name=\"MLP_Output\")(MLP_Layer)\n",
    "\n",
    "    supervised_autoencoder = Model(input_layer, (output_layer, output))\n",
    "\n",
    "    print(\"Using the following loss for Autoencoder: \", autoencoder_loss)\n",
    "    print(\"Using the following metric for Autoencoder: \", autoencoder_metric)\n",
    "    print(\"Using the following loss for MLP: \", mlp_loss)\n",
    "    print(\"Using the following metric for MLP: \", mlp_metric)\n",
    "\n",
    "    supervised_autoencoder.compile(\n",
    "    optimizer=Adam(learning_rate=learning_rate),\n",
    "    loss={\n",
    "        'Autoencoder_Output': [autoencoder_loss],\n",
    "        'MLP_Output': [mlp_loss]\n",
    "    },\n",
    "    metrics={\n",
    "        'Autoencoder_Output': [autoencoder_metric],\n",
    "        'MLP_Output': [mlp_metric]\n",
    "    }\n",
    ")\n",
    "\n",
    "    early_stop_ae = EarlyStopping(monitor='val_Autoencoder_Output_mse', patience=30, mode='min', restore_best_weights=True)\n",
    "    val_MLP_Output_accuracy = EarlyStopping(monitor='val_MLP_Output_accuracy', patience=30, restore_best_weights=True)\n",
    "\n",
    "\n",
    "    # 3. Fit with named inputs/outputs so itâ€™s crystal-clear which target goes where\n",
    "    history = supervised_autoencoder.fit(\n",
    "        x=X_train,\n",
    "        y={\n",
    "            'Autoencoder_Output': X_train,\n",
    "            'MLP_Output':         y_train\n",
    "        },\n",
    "        validation_data=(\n",
    "            X_val,\n",
    "            {\n",
    "                'Autoencoder_Output': X_val,\n",
    "                'MLP_Output':         y_val\n",
    "            }\n",
    "        ),\n",
    "        epochs=num_epochs,\n",
    "        batch_size=32,\n",
    "        shuffle=True,\n",
    "        callbacks=[early_stop_ae, val_MLP_Output_accuracy],\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Get the keys from the history object\n",
    "    keys = history.history.keys()\n",
    "\n",
    "    # Open a text file and write the keys\n",
    "    with open('history_keys.txt', 'w') as file:\n",
    "        for key in keys:\n",
    "            file.write(f\"{key}\\n\")\n",
    "\n",
    "    print(\"Keys have been saved to 'history_keys.txt'.\")\n",
    "\n",
    "    # Create a folder for this trial's model\n",
    "    model_folder = f'{trial_models}/Model_{trial_number}/'\n",
    "    os.makedirs(model_folder, exist_ok=True)\n",
    "\n",
    "    # Save the model\n",
    "    model = f'{model_folder}{trial_number}_Supervised_autoencoder_Churn.h5'\n",
    "    supervised_autoencoder.save(model)\n",
    "    print(f\"Saved model as {model}\")\n",
    "    \n",
    "    # Save the model architecture as an image\n",
    "    save_model_architecture(supervised_autoencoder, trial_number)\n",
    "\n",
    "    # Save the accuracy and loss plots\n",
    "    save_plots(history, trial_number, \"Supervised_autoencoder\")\n",
    "\n",
    "    # Save the hyperparameters used in a text file\n",
    "    save_hyperparameters(trial_number, learning_rate, num_epochs, num_layers_auto, num_layers_mlp, \n",
    "                         nodes_per_layer_autoencoder, nodes_per_layer_mlp, use_dropout, dropout_rate, codeNodes)\n",
    "\n",
    "    return history, supervised_autoencoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d795b03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Function for multi-objective hyperparameter optimization using Optuna.\"\"\"\n",
    "\n",
    "def multi_objective(trial, x_shape):\n",
    "    # Suggest hyperparameters\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-6, 1e-3, log=True)\n",
    "    num_epochs = trial.suggest_int(\"num_epochs\", 1, 500)\n",
    "    num_layers_auto = trial.suggest_int(\"num_layers\", 1, 10)\n",
    "    num_layers_mlp = trial.suggest_int(\"num_layers_mlp\", 1, 10)\n",
    "    use_dropout = trial.suggest_categorical(\"use_dropout\", [True, False])\n",
    "    dropout_rate = trial.suggest_float(\"dropout_rate\", 0.0, 0.5)\n",
    "    \n",
    "    # Traditional Autoencoder\n",
    "    nodes_per_layer_autoencoder = []\n",
    "    upper_bound = 512\n",
    "    \n",
    "    for i in range(num_layers_auto):\n",
    "        # Suggest the number of nodes for the current layer\n",
    "        nodes = trial.suggest_int(f\"nodes_per_layer_autoencoder_{i}\", 2, upper_bound, log=True)\n",
    "        nodes_per_layer_autoencoder.append(nodes)\n",
    "        upper_bound = nodes  # set the upper bound for the next layer to be the number of nodes in the current layer\n",
    "\n",
    "    nodes_per_layer_mlp = []\n",
    "    for i in range(num_layers_mlp):\n",
    "        # Suggest the number of nodes for the current layer\n",
    "        nodes = trial.suggest_int(f\"nodes_per_layer_mlp_{i}\", 2, 512, log=True)\n",
    "        nodes_per_layer_mlp.append(nodes)\n",
    "\n",
    "    minNodes = min(nodes_per_layer_autoencoder)\n",
    "    codeNodes = trial.suggest_int('nodes_in_code', 1, minNodes-1, step=1, log=True)\n",
    "\n",
    "    trial.params['nodes_per_layer_autoencoder'] = nodes_per_layer_autoencoder\n",
    "    trial.params['nodes_per_layer_mlp'] = nodes_per_layer_mlp\n",
    "\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test = data()\n",
    "\n",
    "    # Build supervised autoencoder with suggested hyperparameters\n",
    "    history, supervised_autoencoder = build_supervised_autoencoder(X_train, X_val, y_train, y_val, learning_rate, num_epochs, num_layers_auto, num_layers_mlp ,nodes_per_layer_autoencoder, nodes_per_layer_mlp ,trial.number, use_dropout, dropout_rate, codeNodes)\n",
    "\n",
    "    val_accuracy = np.mean(history.history['val_MLP_Output_accuracy'])\n",
    "    MLP_loss = np.mean(history.history['val_MLP_Output_loss'])\n",
    "    autoencoder_loss = np.mean(history.history['val_Autoencoder_Output_loss'])\n",
    "\n",
    "    # Save the trial number and the hyperparameters in a file\n",
    "    with open(trials + \"Churn_Trials.txt\", \"a\") as f:\n",
    "        f.write(\"Trial number:\" + str(trial.number) + \"\\n\")\n",
    "        f.write(\"Learning rate:\" + str(learning_rate) + \"\\n\")\n",
    "        f.write(\"Number of epochs:\" + str(num_epochs) + \"\\n\")\n",
    "        f.write(\"Number of Autoencoder layers:\" + str(num_layers_auto) + \"\\n\")\n",
    "        f.write(\"Nodes per layer Autoencoder:\" + str(nodes_per_layer_autoencoder) + \"\\n\")\n",
    "        f.write(\"Nodes in code layer:\" + str(codeNodes) + \"\\n\")\n",
    "        f.write(\"Number of MLP layers:\" + str(num_layers_mlp) + \"\\n\")\n",
    "        f.write(\"Nodes per layer MLP:\" + str(nodes_per_layer_mlp) + \"\\n\")\n",
    "        f.write(\"Use dropout:\" + str(use_dropout) + \"\\n\")\n",
    "        f.write(\"Dropout rate:\" + str(dropout_rate) + \"\\n\")\n",
    "        f.write(\"Validation accuracy:\" + str(val_accuracy) + \"\\n\")\n",
    "        f.write(\"MLP_loss:\" + str(MLP_loss) + \"\\n\")\n",
    "        f.write(\"autoencoder_loss:\" + str(autoencoder_loss) + \"\\n\")\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "    overall_Loss = MLP_loss + autoencoder_loss\n",
    "\n",
    "    # Store them in user attributes for later plotting\n",
    "    trial.set_user_attr(\"val_MLP_Output_accuracy\", val_accuracy)\n",
    "    trial.set_user_attr(\"val_MLP_Output_loss\", MLP_loss)\n",
    "    \n",
    "    return overall_Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb60512b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Main function to build and train the model with hyperparameter optimization. \"\"\"\n",
    "\n",
    "def ModelBuildAndTrain():\n",
    "    run_data_prep_part2()\n",
    "    \n",
    "    supervised_autoencoder = None\n",
    "\n",
    "    # Load data to get the shape of X_train\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test = data()\n",
    "    x_shape = X_train.shape[1]\n",
    "\n",
    "    # Create a single-objective Optuna study\n",
    "    study = optuna.create_study(direction='minimize', study_name=\"Supervised Autoencoder\")\n",
    "    study.optimize(lambda trial: multi_objective(trial, x_shape), n_trials=amount_of_trials)\n",
    "    print(\"Number of finished trials: \", len(study.trials))\n",
    "\n",
    "    # Retrieve the best trial\n",
    "    best_trial = study.best_trial\n",
    "\n",
    "    # Save best trial data to a text file\n",
    "    with open(results + 'Best_Trials.txt', 'w') as file:\n",
    "        file.write(f\"Best Trial Number: {best_trial.number}\\n\")\n",
    "        file.write(f\"Best Trial Value: {best_trial.value}\\n\")\n",
    "        file.write(\"Best Trial Parameters:\\n\")\n",
    "        for key, value in best_trial.params.items():\n",
    "            file.write(f\"  {key}: {value}\\n\")\n",
    "\n",
    "    # Extract validation accuracy and loss from user_attrs\n",
    "    val_accuracies = [t.user_attrs.get(\"val_MLP_Output_accuracy\") for t in study.trials]\n",
    "    val_losses = [t.user_attrs.get(\"val_MLP_Output_loss\") for t in study.trials]\n",
    "\n",
    "    # Plot Validation Accuracy\n",
    "    fig_acc = plt.figure(figsize=(10, 5))\n",
    "    plt.plot(range(len(val_accuracies)), val_accuracies, marker='o')\n",
    "    plt.title('Optuna Study - Validation Accuracy per Trial')\n",
    "    plt.xlabel('Trial Number')\n",
    "    plt.ylabel('Validation Accuracy')\n",
    "    plt.grid(True)\n",
    "    fig_acc.savefig(model + 'study_results_accuracy.png')\n",
    "\n",
    "    # Plot Validation Loss\n",
    "    fig_loss = plt.figure(figsize=(10, 5))\n",
    "    plt.plot(range(len(val_losses)), val_losses, marker='o')\n",
    "    plt.title('Optuna Study - Validation Loss per Trial')\n",
    "    plt.xlabel('Trial Number')\n",
    "    plt.ylabel('Validation Loss')\n",
    "    plt.grid(True)\n",
    "    fig_loss.savefig(model + 'study_results_loss.png')\n",
    "\n",
    "    best_model_path = trial_models + f'Model_{best_trial.number}\\{best_trial.number}_Supervised_autoencoder_Churn.h5'\n",
    "\n",
    "\n",
    "    return best_model_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7969d258",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    \n",
    "    best_model_path = ModelBuildAndTrain()\n",
    "\n",
    "    print(\"Best model path:\", best_model_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Masters",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
